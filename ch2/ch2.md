# 第二章 模型评估与选择

## 2.1 经验误差与过拟合

通常我们把分类错误的样本数占样本总数的比例称为 **错误率**，即如果在m个样本中有a个样本分类错误，则错误率 **E = a/m**; 相应的， **1-a/m** 称为 **精度**
更一般地，我们把学习器地实际预测输出与样本的真实输出之间的差异称为 **误差**， 学习器在训练集上的误差称为 **训练误差** 或 **经验误差**，在新样本上的误差称为 **泛化误差**
学习器有可能把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，这样就会导致泛化能力下降，这种现象在机器学习中称为 **过拟合**，与之相对的是 **欠拟合**，这是指对训练样本的一般性质尚未学好

## 2.2 评估方法

通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个 **测试集** 来擦拭学习器对新样本的判别能力，然后以测试集上的 **测试误差**作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需要注意的是，**测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过**

以下为几种常用的获取训练集和测试集的方法

### 2.2.1 留出法

**留出法** 直接将数据集 $D$ 划分为两个 **互斥** 的集合，其中一个集合作为训练集 $S$，另一个作为测试集 $T$，即 $D=S \cup T$, $S \cap T=\emptyset$, 在 $S$ 上训练出模型后，用 $T$ 来评估其测试误差，作为对泛化误差的估计
需要注意的是，训练/测试集的划分要尽可能保持 **数据分布的一致性**，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例类似
如果从采样的角度来看待数据集的划分过程，则**保留类别比例**的采样方式通常称为 **分层采样**
单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用 **若干次随机划分、重复进行实验评估后取平均值** 作为留出法的评估结果
常见做法是，大约2/3 ~ 4/5的样本用于训练，剩余样本用于测试，测试集一般包含不少于30个样例

### 2.2.2 交叉验证法

**交叉验证法** 先将数据集 $D$ 划分为 k 个 **大小相似的互斥子集**，即 $D=D_1\cup D_2\cup ...\cup D_k,D_i\cap D_j=\emptyset (i\neq j) $ 每个子集 $D_i$ 都尽可能保持数据分布的一致性，即从 $D$ 中通过 **分层采样** 得到。然后，每次用 $k-1$ 个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得 $k$ 组训练/测试集，从而可进行 $k$ 次训练和测试，最终返回的是这 $k$ 个测试结果的均值
显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 $k$ 的取值，为强调这一点，通常把交叉验证法称为 **k折交叉验证**

### 2.2.3 自助法

**自助法** 直接以 **自助采样法** 为基础，给定包含m个样本的数据集 $D$ ，我们对它进行采样产生数据集 $D'$ ：每次随机从 $D$ 中挑选一个样本将其拷贝放入 $D'$ ，然后再将该样本放回初始数据集 $D$ 中，使得该样本在下次采样中仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集 $D'$
显然， $D$ 中的一部分样本会在 $D'$ 中多次出现，而另一部分样本不出现，经分析，样本在m次采样中始终不被采到的概率为 $1/e$，因此，我们可以将 $D'$ 用作训练集， $D-D'$ 用作测试集，这样的测试结果，亦称 **包外估计**

### 2.2.4 调参与最终模型

大多数学习算法都有些参数需要设定，参数配置不同，学得模型的性能往往有显著差别，因此，在进行模型评估和选择时，除了要对适用学习算法进行选择，还要对算法参数进行设定，这就是通常所说的 **参数调节** 

## 2.3 性能度量

对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方案，还要有衡量模型泛化能力的评价标准，这就是 **性能度量**

### 2.3.1 错误率与精度

错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，对样例集 $D$ ，分类错误率定义为 $$ E(f;D) = \frac{1}{m} \sum_{i=1}^mII(f(x_i)\neq y_1)$$ 精度则定义为
$$acc(f;D) = 1-E(f;D)$$

### 2.3.2 查准率、查全率与F1

对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例、假正例、真反例、假反例四种情形，分类结果的“混淆矩阵”如下所示：
||正例|反例|
|:-:|:-:|:-:|
|正例|TP|FN|
|反例|FP|TN|
查准率P和查全率R分别定义为
$$P=\frac{TP}{TP+FP}$$
$$R=\frac{TP}{TP+FN}$$
一般来说，查准率高时，查全率往往偏低，反之亦然。
以查准率为纵轴，查全率为横轴作图，就可以得到 **P-R曲线**，显示该曲线的图称为 **P-R图**
如果一个学习器的P-R曲线被另一个曲线完全包住，则可断言后者的学习性能优于前者
为了综合考虑查准率、查全率的性能度量，引入 **平衡点（BEP）** 概念，其为查准率=查全率时的取值，一般认为BEP越大，学习器的学习性能越好

更常用的是F1度量：$$F1=\frac{2\times P\times R}{P + R}=\frac{2\times TP}{样例总数+TP-TN}$$
此时查准率和查全率平权，为了自定义查准率和查全率的权重，引入 $F_\beta$,其定义为：$$F_\beta = \frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P)+R}$$
$\beta$>1时查全率更大影响，$\beta$<1时查准率更大影响